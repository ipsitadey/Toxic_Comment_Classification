INFO:p_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/vikashyadav/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:p_transformers.modeling_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": "binary",
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:p_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/vikashyadav/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
INFO:p_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/vikashyadav/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
INFO:p_transformers.modeling_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:p_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/vikashyadav/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
INFO:p_transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['bert.encoder.layer.0.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.0.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.0.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.0.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.0.output.dense_adapter_1.weight', 'bert.encoder.layer.0.output.dense_adapter_1.bias', 'bert.encoder.layer.0.output.dense_adapter_2.weight', 'bert.encoder.layer.0.output.dense_adapter_2.bias', 'bert.encoder.layer.1.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.1.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.1.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.1.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.1.output.dense_adapter_1.weight', 'bert.encoder.layer.1.output.dense_adapter_1.bias', 'bert.encoder.layer.1.output.dense_adapter_2.weight', 'bert.encoder.layer.1.output.dense_adapter_2.bias', 'bert.encoder.layer.2.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.2.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.2.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.2.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.2.output.dense_adapter_1.weight', 'bert.encoder.layer.2.output.dense_adapter_1.bias', 'bert.encoder.layer.2.output.dense_adapter_2.weight', 'bert.encoder.layer.2.output.dense_adapter_2.bias', 'bert.encoder.layer.3.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.3.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.3.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.3.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.3.output.dense_adapter_1.weight', 'bert.encoder.layer.3.output.dense_adapter_1.bias', 'bert.encoder.layer.3.output.dense_adapter_2.weight', 'bert.encoder.layer.3.output.dense_adapter_2.bias', 'bert.encoder.layer.4.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.4.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.4.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.4.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.4.output.dense_adapter_1.weight', 'bert.encoder.layer.4.output.dense_adapter_1.bias', 'bert.encoder.layer.4.output.dense_adapter_2.weight', 'bert.encoder.layer.4.output.dense_adapter_2.bias', 'bert.encoder.layer.5.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.5.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.5.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.5.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.5.output.dense_adapter_1.weight', 'bert.encoder.layer.5.output.dense_adapter_1.bias', 'bert.encoder.layer.5.output.dense_adapter_2.weight', 'bert.encoder.layer.5.output.dense_adapter_2.bias', 'bert.encoder.layer.6.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.6.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.6.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.6.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.6.output.dense_adapter_1.weight', 'bert.encoder.layer.6.output.dense_adapter_1.bias', 'bert.encoder.layer.6.output.dense_adapter_2.weight', 'bert.encoder.layer.6.output.dense_adapter_2.bias', 'bert.encoder.layer.7.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.7.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.7.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.7.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.7.output.dense_adapter_1.weight', 'bert.encoder.layer.7.output.dense_adapter_1.bias', 'bert.encoder.layer.7.output.dense_adapter_2.weight', 'bert.encoder.layer.7.output.dense_adapter_2.bias', 'bert.encoder.layer.8.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.8.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.8.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.8.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.8.output.dense_adapter_1.weight', 'bert.encoder.layer.8.output.dense_adapter_1.bias', 'bert.encoder.layer.8.output.dense_adapter_2.weight', 'bert.encoder.layer.8.output.dense_adapter_2.bias', 'bert.encoder.layer.9.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.9.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.9.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.9.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.9.output.dense_adapter_1.weight', 'bert.encoder.layer.9.output.dense_adapter_1.bias', 'bert.encoder.layer.9.output.dense_adapter_2.weight', 'bert.encoder.layer.9.output.dense_adapter_2.bias', 'bert.encoder.layer.10.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.10.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.10.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.10.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.10.output.dense_adapter_1.weight', 'bert.encoder.layer.10.output.dense_adapter_1.bias', 'bert.encoder.layer.10.output.dense_adapter_2.weight', 'bert.encoder.layer.10.output.dense_adapter_2.bias', 'bert.encoder.layer.11.attention.output.dense_adapter_1.weight', 'bert.encoder.layer.11.attention.output.dense_adapter_1.bias', 'bert.encoder.layer.11.attention.output.dense_adapter_2.weight', 'bert.encoder.layer.11.attention.output.dense_adapter_2.bias', 'bert.encoder.layer.11.output.dense_adapter_1.weight', 'bert.encoder.layer.11.output.dense_adapter_1.bias', 'bert.encoder.layer.11.output.dense_adapter_2.weight', 'bert.encoder.layer.11.output.dense_adapter_2.bias', 'classifier.weight', 'classifier.bias']
INFO:p_transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
INFO:__main__:Creating features from dataset file at ../input/jigsaw-unintended-bias-in-toxicity-classification/
INFO:__main__:Saving features into cached file ../input/jigsaw-unintended-bias-in-toxicity-classification/cached_train_bert-base-uncased_128_binary
INFO:__main__:***** Running training *****
INFO:__main__:  Num examples = 898
INFO:__main__:  Num Epochs = 1
INFO:__main__:  Total train batch size  = 8
INFO:__main__:  Gradient Accumulation steps = 1
INFO:__main__:  Total optimization steps = 113
INFO:__main__: global_step = 113, average loss = 0.25437679130221363
INFO:__main__:Saving model checkpoint to ../outputs/
INFO:__main__:Evaluate the following checkpoints: ['../outputs']
INFO:p_transformers.modeling_utils:loading configuration file ../outputs/config.json
INFO:p_transformers.modeling_utils:Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

INFO:p_transformers.modeling_utils:loading weights file ../outputs/pytorch_model.bin
INFO:__main__:Creating features from dataset file at ../input/jigsaw-unintended-bias-in-toxicity-classification/
INFO:__main__:Saving features into cached file ../input/jigsaw-unintended-bias-in-toxicity-classification/cached_dev_bert-base-uncased_128_binary
INFO:__main__:***** Running evaluation  *****
INFO:__main__:  Num examples = 99
INFO:__main__:  Batch size = 8
INFO:__main__:***** Eval results  *****
INFO:__main__:  fn = 6
INFO:__main__:  fp = 0
INFO:__main__:  mcc = 0.0
INFO:__main__:  tn = 93
INFO:__main__:  tp = 0
